
@article{tibshirani_regression_1996,
	title = {Regression {Shrinkage} and {Selection} via the {Lasso}},
	volume = {58},
	issn = {00359246},
	url = {http://www.jstor.org/stable/2346178},
	abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
	number = {1},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Tibshirani, Robert},
	year = {1996},
	pages = {267--288},
	file = {[Tibshirani.1996] Regression Shrinkage and Selection via the Lasso.pdf:/home/kisung/Dropbox/V2. Applied Mathematics/P2. Numerical/S2. Sparsity/01. LASSO/[Tibshirani.1996] Regression Shrinkage and Selection via the Lasso.pdf:application/pdf}
}

@book{hastie_statistical_2015,
	address = {Boca Raton},
	series = {Monographs on statistics and applied probability},
	title = {Statistical learning with sparsity: the lasso and generalizations},
	isbn = {978-1-4987-1216-3},
	shorttitle = {Statistical learning with sparsity},
	number = {143},
	publisher = {CRC Press, Taylor \& Francis Group},
	author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
	year = {2015},
	keywords = {Mathematical statistics, Least squares, Linear models (Statistics), Proof theory},
	file = {[Hastie.2015] Statistical learning with sparsity.pdf:/home/kisung/Dropbox/V2. Applied Mathematics/P2. Numerical/S2. Sparsity/[Hastie.2015] Statistical learning with sparsity.pdf:application/pdf}
}

@article{ma_alternating_2013,
	title = {Alternating {Direction} {Method} of {Multipliers} for {Sparse} {Principal} {Component} {Analysis}},
	volume = {1},
	issn = {2194-668X, 2194-6698},
	url = {http://link.springer.com/10.1007/s40305-013-0016-9},
	doi = {10.1007/s40305-013-0016-9},
	language = {en},
	number = {2},
	urldate = {2018-02-01},
	journal = {Journal of the Operations Research Society of China},
	author = {Ma, Shiqian},
	month = jun,
	year = {2013},
	pages = {253--274},
	file = {[Ma.2013] Alternating Direction Method of Multipliers for Sparse Principal Component.pdf:/home/kisung/Dropbox/V3. Statistics/P3. Dimension Reduction/S1. Linear/18. SPCA/[Ma.2013] Alternating Direction Method of Multipliers for Sparse Principal Component.pdf:application/pdf}
}

@article{candes_robust_2011,
	title = {Robust principal component analysis?},
	volume = {58},
	issn = {00045411},
	url = {http://portal.acm.org/citation.cfm?doid=1970392.1970395},
	doi = {10.1145/1970392.1970395},
	language = {en},
	number = {3},
	urldate = {2017-12-03},
	journal = {Journal of the ACM},
	author = {Candès, Emmanuel J. and Li, Xiaodong and Ma, Yi and Wright, John},
	month = may,
	year = {2011},
	pages = {1--37},
	file = {[Candès.2011] Robust principal component analysis.pdf:/home/kisung/Dropbox/V3. Statistics/P3. Dimension Reduction/S2. Nonlinear/08. RPCA/[Candès.2011] Robust principal component analysis.pdf:application/pdf}
}
